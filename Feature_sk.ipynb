{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_sk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/crispy-fiesta/blob/main/Feature_sk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012IjyNqMK1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbb07ed-e06c-4ea6-ae0e-67081f3020e2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_hJ2-On_8X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGy8MxtKlWxv"
      },
      "source": [
        "train = pd.read_csv('./train.csv',engine='python')\n",
        "test = pd.read_csv('./test.csv',engine='python')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHibPzaDTeVo"
      },
      "source": [
        "X_train = train.body  # train texts\n",
        "y_train = train.subreddit # train subreddits\n",
        "X_test = test.body  # test texts"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXCloN6pnqQr"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2WEwNHdAfM"
      },
      "source": [
        "###Processing based on sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJTvw3DMtG4"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer, LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2vtasvRWkeM",
        "outputId": "b0cc1bb5-3445-4e8a-c36c-72b84c5b2c4a"
      },
      "source": [
        "# transform target labels to values\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train.values)\n",
        "\n",
        "# vectorize word count\n",
        "vectorizer = CountVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(X_train)\n",
        "vectors_test = vectorizer.transform(X_test)\n",
        "vectors_train = vectors_train.todense()\n",
        "vectors_test = vectors_test.todense()\n",
        "\n",
        "# onehot encoding\n",
        "onehot = OneHotEncoder(handle_unknown = 'ignore')\n",
        "vectors_train = onehot.fit_transform(vectors_train)\n",
        "vectors_test = onehot.transform(vectors_test)\n",
        "\n",
        "# print(vectorizer.get_feature_names())\n",
        "print(vectors_train.shape)\n",
        "print(vectors_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 35729)\n",
            "(1378, 35729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lMh3F7M3SuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5982fbb8-cc4f-4be5-d340-89694089a25f"
      },
      "source": [
        "# remove stop words\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words)\r\n",
        "vectors_train_stop = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "normalizer_train = Normalizer()\r\n",
        "vectors_train_stop= normalizer_train.transform(vectors_train_stop)\r\n",
        "vectors_test_stop = normalizer_train.transform(vectors_test_stop)\r\n",
        "print(vectors_train_stop.shape)\r\n",
        "print(vectors_test_stop.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15079)\n",
            "(1378, 15079)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llzfTbCj3svy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1447a80-832c-4ee1-d0de-d766bf66369f"
      },
      "source": [
        "# tf-idf\r\n",
        "tf_idf_vectorizer = TfidfVectorizer()\r\n",
        "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\r\n",
        "vectors_train_idf= normalizer_train.transform(vectors_train_idf)\r\n",
        "vectors_test_idf = normalizer_train.transform(vectors_test_idf)\r\n",
        "print(vectors_train_idf.shape)\r\n",
        "print(vectors_test_idf.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15365)\n",
            "(1378, 15365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z915xw1Hc6s9"
      },
      "source": [
        "### Processing based on nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDORAHscEv-",
        "outputId": "c7c7a88e-fb1e-40e9-ac67-6065d540627d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8yUINtfBzX"
      },
      "source": [
        "####Stemming\n",
        "features: `vector_train_stem`, `vector_test_stem`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XpLsfP4qiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80278c71-2285-4803-82f4-7e51369a3726"
      },
      "source": [
        "# stemming\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\r\n",
        "vectors_train_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stem= normalizer_train.transform(vectors_train_stem)\r\n",
        "vectors_test_stem = normalizer_train.transform(vectors_test_stem)\r\n",
        "print(vectors_train_stem.shape)\r\n",
        "print(vectors_test_stem.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 8727)\n",
            "(1378, 8727)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lDtjGaCe9mt"
      },
      "source": [
        "#### Lemmatization\n",
        "features: `vector_train_Lemma`, `vector_test_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9TOxOl5WsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f7698b-fac3-49d2-9782-43992dac89b6"
      },
      "source": [
        "# Lemmatization\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "  \r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)\r\n",
        "vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)\r\n",
        "print(vectors_train_Lemma.shape)\r\n",
        "print(vectors_test_Lemma.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 10045)\n",
            "(1378, 10045)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbRxM6qe5n9"
      },
      "source": [
        "#### Put it all together\n",
        "features: `vector_train_stop_Lemma`, `vector_test_stop_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xyxvw6QAEP8",
        "outputId": "db935142-0e0f-419f-8a13-f782a2c6c476"
      },
      "source": [
        "# put it all together: remove stopwords, tfidf, punctuation, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words,tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_stop_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_Lemma = tf_idf_transformer.fit_transform(vectors_train_stop_Lemma)\r\n",
        "\r\n",
        "vectors_test_stop_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_Lemma = tf_idf_transformer.transform(vectors_test_stop_Lemma)\r\n",
        "vectors_train_stop_Lemma = normalizer_train.transform(vectors_train_stop_Lemma)\r\n",
        "vectors_test_stop_Lemma = normalizer_train.transform(vectors_test_stop_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_Lemma.shape)\r\n",
        "print(vectors_test_stop_Lemma.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymF0ztmLnbGq"
      },
      "source": [
        "## Experiments with models in sk-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH-sQOatGy54"
      },
      "source": [
        "vectors_train，vectors_test是加了onehot的，vectors_train_stop_Lemma是把几种处理都加上的，target是y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PccMQje5G0wq"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMEu17OWG_qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0b0ec6-b06f-4493-911e-398feed544a2"
      },
      "source": [
        "X_train = vectors_train_stop_Lemma\n",
        "y_train = y_train\n",
        "X_test = vectors_test_stop_Lemma\n",
        "# y_test unknown\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9409704852426213"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-2Gt9mxNfyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba43889f-7512-4145-d4d6-3fdcf0c876aa"
      },
      "source": [
        "X_train = vectors_train\n",
        "y_train = y_train\n",
        "X_test = vectors_test\n",
        "# y_test unknown\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9074537268634317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5JthkuOxfP"
      },
      "source": [
        "### Kfold cv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym2wjVFJPyhl"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "model = LinearSVC()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjLrCRIVOwn-"
      },
      "source": [
        "X = vectors_train_stop_Lemma\n",
        "y = y_train\n",
        "# X_test = vectors_test_stop_Lemma\n",
        "# y_test unknown"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Lb1SuWNzg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb3828a-7bff-43ad-9b53-f5990e6b652d"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"test accu: \", model.score(X_test, y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accu:  0.94\n",
            "test accu:  0.935\n",
            "test accu:  0.91\n",
            "test accu:  0.9\n",
            "test accu:  0.955\n",
            "test accu:  0.965\n",
            "test accu:  0.955\n",
            "test accu:  0.91\n",
            "test accu:  0.93\n",
            "test accu:  0.9447236180904522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvT3RTzGgxk"
      },
      "source": [
        "## Bernoulli NB model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUmRiQt-oF5e"
      },
      "source": [
        "import time\n",
        "import sklearn.preprocessing as skpre\n",
        "# from sklearn import metrics\n",
        "# from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9EvhfoXqr2M"
      },
      "source": [
        "class Bernoulli_NB():\r\n",
        "  def __init__(self, LaplaceSmoothing = True):\r\n",
        "    self.LaplaceSmoothing = LaplaceSmoothing\r\n",
        "    self.Prob_Y = None      # P(Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.n_class = 0\r\n",
        "    self.w0 = None\r\n",
        "    self.w = None\r\n",
        "    self.Ytarget = None\r\n",
        "\r\n",
        "  def encoder(X):\r\n",
        "    return skpre.OneHotEncoder(X)\r\n",
        "\r\n",
        "  def ProbY(self, Y):\r\n",
        "    # calculate P(Y=1) and P(Y=0)\r\n",
        "    ProbY = np.zeros((1,2))\r\n",
        "    #print('shape',np.shape(Y))\r\n",
        "    ProbY[0,1] = np.sum(Y)/np.shape(Y)\r\n",
        "    ProbY[0,0] = 1 - ProbY[0,1]\r\n",
        "    return ProbY\r\n",
        "\r\n",
        "  def ProbX_Yi(self, X, Y, label):\r\n",
        "    # calculte P(xj=1|Yi=1)\r\n",
        "    rows,cols = X.shape       # feature shape\r\n",
        "    numerator = np.zeros(cols)    # initialize numerator\r\n",
        "\r\n",
        "    # xj=1 and Yi=1\r\n",
        "    for n in range(rows):\r\n",
        "      if Y[n] == label:\r\n",
        "        numerator += X[n,:]\r\n",
        "    # Yi=1\r\n",
        "    denominator = np.count_nonzero(Y == label)\r\n",
        "    # Laplace Smoothing\r\n",
        "    if(self.LaplaceSmoothing):\r\n",
        "      numerator += 1\r\n",
        "      denominator += 2\r\n",
        "    # P(xj=1|Yi=1)\r\n",
        "    prob = numerator/denominator\r\n",
        "    return prob    \r\n",
        "\r\n",
        "  def fit(self, X, Y):\r\n",
        "    print('---------------------- start fitting ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    X = self.encoder(X)\r\n",
        "    rows,cols = X.shape              # feature shape\r\n",
        "    self.n_class = len(np.unique(Y))       # number of classes\r\n",
        "    self.Prob_Y = np.zeros((self.n_class,2))   # initialize P(Y)\r\n",
        "    self.Prob_X_Y = np.zeros((self.n_class,2,cols)) # initialize P(x|Y)\r\n",
        "    c = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    d = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    self.w0 = np.zeros((1,self.n_class))     # [w0Y1,w0Y2,...]\r\n",
        "    self.w = np.zeros((self.n_class,cols))    # [(w1,w2,...)Y1;\r\n",
        "                            # (w1,w2,...)Y2]\r\n",
        "    self.Ytarget = np.unique(Y)\r\n",
        "    Y_index = 0\r\n",
        "    for Yi in self.Ytarget:\r\n",
        "      Y_onevsall = np.where(Y == Yi, 1, 0)    # only have 2 classes: Yi(1) & notYi(0)\r\n",
        "      self.Prob_Y[Y_index,:] = self.ProbY(Y_onevsall)   # [P(notYi), P(Yi)]\r\n",
        "      self.Prob_X_Y[Y_index,0,:] = self.ProbX_Yi(X,Y_onevsall,0)  # [P(x1|notYi), P(x2|notYi),...]\r\n",
        "      self.Prob_X_Y[Y_index,1,:] = self.ProbX_Yi(X,Y_onevsall,1)  # [P(x1|Yi), P(x2|Yi),...]\r\n",
        "      c[Y_index,:] = np.log10(self.Prob_X_Y[Y_index,1,:]/self.Prob_X_Y[Y_index,0,:])     # log(P(xj|Y=1)/P(xj|Y=0))\r\n",
        "      d[Y_index,:] = np.log10((1-self.Prob_X_Y[Y_index,1,:])/(1-self.Prob_X_Y[Y_index,0,:])) # log((1-P(xj|Y=1))/(1-P(xj|Y=0)))\r\n",
        "      self.w0[0,Y_index] = np.log10(self.Prob_Y[Y_index,1]/self.Prob_Y[Y_index,0]) + np.sum(d[Y_index,:])\r\n",
        "      self.w[Y_index,:] = c[Y_index,:] - d[Y_index,:]\r\n",
        "      Y_index += 1\r\n",
        "\r\n",
        "    print('------ fit done, total time: ',time.time()-t1,' -----')\r\n",
        "    # return self.Prob_Y,self.Prob_X_Y\r\n",
        "\r\n",
        "  def predict(self, X):\r\n",
        "    print('---------------------- start predict ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    X = self.encoder(X)\r\n",
        "    rows,cols = np.shape(X)       # feature shape\r\n",
        "    PreY = np.empty((rows,1),dtype=str) # initialize Y\r\n",
        "    # print('type',type(PreY),'pre',PreY)\r\n",
        "    LogOddsRatio = np.zeros((1,self.n_class))  # initialize log odds ratio a(x) \r\n",
        "    Logistic = np.zeros((1,self.n_class))    # initialize logistic function\r\n",
        "    Y_index = 0\r\n",
        "    for obs in range(rows):\r\n",
        "      for Yi in self.Ytarget:\r\n",
        "        #print('w0 type:',type(self.w0[0,Y_index]))\r\n",
        "        #print('w shape:',np.shape(self.w[Y_index,:].reshape(1,cols)))\r\n",
        "        #print('X shape:',np.shape(np.transpose(X[obs,:])))\r\n",
        "        #print('part2:',np.dot(self.w[Y_index,:].reshape(1,cols),np.transpose(X[obs,:])))\r\n",
        "        #LogOddsRatio[0,Y_index] = self.w0[0,Y_index] + np.dot(self.w[Y_index,:].reshape(1,cols),np.transpose(X[obs,:]))\r\n",
        "        \r\n",
        "        part2 = self.w[Y_index,:].reshape(1,cols) @ np.transpose(X[obs,:])\r\n",
        "        #print('part2:',part2.astype(np.float64))\r\n",
        "        LogOddsRatio[0,Y_index] = self.w0[0,Y_index] + part2.astype(np.float64)\r\n",
        "        Logistic[0,Y_index] = 1/(1+np.exp(-LogOddsRatio[0,Y_index]))\r\n",
        "        Y_index += 1\r\n",
        "      Y_index = 0\r\n",
        "      # print(np.where(Logistic == np.amax(Logistic))[1])\r\n",
        "      PreY[obs] = self.Ytarget[np.where(Logistic == np.amax(Logistic))[1]]\r\n",
        "    print('------ predict done, total time: ',time.time()-t1,' -----')\r\n",
        "    return PreY\r\n",
        "\r\n",
        "  def score(self,X,Y):\r\n",
        "    # Return the mean accuracy on the given test data and labels.\r\n",
        "    PreY = self.predict(X)\r\n",
        "    rows = np.shape(Y)[0]\r\n",
        "    n_correct = 0;\r\n",
        "    for obs in range(rows):\r\n",
        "      if PreY[obs] == Y[obs]:\r\n",
        "        n_correct += 1\r\n",
        "    #print('n:',n_correct)\r\n",
        "    #print('rows:',rows)\r\n",
        "    accuracy = n_correct / rows\r\n",
        "    print('------------------ accuracy:',accuracy,' -----------------')\r\n",
        "    return accuracy\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXA3IZnjrb2j"
      },
      "source": [
        "X = vectors_train_stop_Lemma\n",
        "y = y_train\n",
        "nb = Bernoulli_NB()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "Dz-zkvYdrOC0",
        "outputId": "b159c329-a40d-479e-d47e-7706b025d7c8"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    nb.fit(X_train, y_train)\n",
        "    print(\"test accu: \", nb.score(X_test, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-78da16ff2e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test accu: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1039\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_seq_items\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"display.width\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                     raise KeyError(\n\u001b[0;32m-> 1316\u001b[0;31m                         \u001b[0;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m                         \u001b[0;34m\"is no longer supported. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                         \u001b[0;34mf\"The following labels were missing: {not_found}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([  15,   22,   23,   43,   63,\\n            ...\\n            1942, 1952, 1954, 1955, 1961],\\n           dtype='int64', length=175). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\""
          ]
        }
      ]
    }
  ]
}